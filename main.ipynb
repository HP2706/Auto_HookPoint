{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/ml/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "from inspect import isclass\n",
    "from torch import nn\n",
    "from transformer_lens.hook_points import HookPoint, HookedRootModule\n",
    "from typing import List, Optional, TypeVar, Type, Union, cast, overload\n",
    "from utils import iterate_module\n",
    "from abc import ABC, abstractmethod\n",
    "import torch\n",
    "from functools import partial\n",
    "from fastcore.basics import *\n",
    "from fastcore.foundation import *\n",
    "from torch import nn\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "from typing import TypeVar, Generic, Union, Type, Any, Callable, get_type_hints, ParamSpec, Protocol\n",
    "from inspect import isclass, signature\n",
    "import functools\n",
    "from fastapi import FastAPI\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" \\nclass AutoHookedRootModule(HookedRootModule):\\n    '''\\n    This class automatically builds hooks for all modules that are not hooks.\\n    NOTE this does not mean all edges in the graph are hooked only that the outputs of the modules are hooked.\\n    for instance torch.softmax(x) is not hooked but self.softmax(x) would be\\n    ''' \""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" \n",
    "class AutoHookedRootModule(HookedRootModule):\n",
    "    '''\n",
    "    This class automatically builds hooks for all modules that are not hooks.\n",
    "    NOTE this does not mean all edges in the graph are hooked only that the outputs of the modules are hooked.\n",
    "    for instance torch.softmax(x) is not hooked but self.softmax(x) would be\n",
    "    ''' \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ModelTest(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bla = nn.ModuleList([nn.Linear(10, 10)])\n",
    "        self.lala = nn.Linear(10, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if isinstance(self, AutoHookedRootModule):\n",
    "            print(f'{self.__class__.__name__}.mod_dict', self.mod_dict)\n",
    "            print(self.bla[0], self.bla[0].hook_dict)\n",
    "        x = self.bla[0].forward(x)\n",
    "        x = self.lala.forward(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "T = TypeVar('T', bound=nn.Module)\n",
    "P = ParamSpec('P')\n",
    "_T = TypeVar(\"_T\", bound=Callable)\n",
    "\n",
    "\n",
    "def same_definition_as_in(t: _T) -> Callable[[Callable], _T]:\n",
    "    def decorator(f: Callable) -> _T:\n",
    "        return f  # type: ignore\n",
    "\n",
    "    return decorator\n",
    "\n",
    "class MyFastAPI(FastAPI):\n",
    "    @same_definition_as_in(FastAPI.__init__)\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs) \n",
    "\n",
    "    @same_definition_as_in(FastAPI.get)\n",
    "    def get(self, *args, **kwargs):\n",
    "        print('get')\n",
    "        return super().get(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('linear1.wtf.wtf.hook_point', HookPoint()), ('linear1.wtf.hook_point', HookPoint()), ('linear1.hook_point', HookPoint()), ('linear2.wtf.wtf.hook_point', HookPoint()), ('linear2.wtf.hook_point', HookPoint()), ('linear2.hook_point', HookPoint()), ('hook_point', HookPoint())]\n",
      "hook_fn CALLED tensor([[0.7629]], grad_fn=<AddmmBackward0>) linear1.wtf.wtf.hook_point\n",
      "hook_fn CALLED tensor([[0.7629]], grad_fn=<AddmmBackward0>) linear1.wtf.hook_point\n",
      "hook_fn CALLED tensor([[0.7629]], grad_fn=<AddmmBackward0>) linear1.hook_point\n",
      "hook_fn CALLED tensor([[0.1865]], grad_fn=<AddmmBackward0>) linear2.wtf.wtf.hook_point\n",
      "hook_fn CALLED tensor([[0.1865]], grad_fn=<AddmmBackward0>) linear2.wtf.hook_point\n",
      "hook_fn CALLED tensor([[0.1865]], grad_fn=<AddmmBackward0>) linear2.hook_point\n",
      "hook_fn CALLED tensor([[0.1865]], grad_fn=<AddmmBackward0>) hook_point\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.1865]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from typing import Optional, Set\n",
    "\n",
    "\n",
    "from torch.nn.modules.module import Module\n",
    "\n",
    "\n",
    "T = TypeVar('T', bound=nn.Module)\n",
    "P = ParamSpec('P')\n",
    "R = TypeVar('R')\n",
    "\n",
    "\n",
    "class WrappedClass(Generic[T]):\n",
    "    def __init__(self, module_class: Type[T]) -> T: # type: ignore\n",
    "        self.module_class = module_class\n",
    "\n",
    "    def __call__(self, *args: Any, **kwargs: Any) -> WrappedInstance[T]:\n",
    "        instance = self.module_class(*args, **kwargs)\n",
    "        return auto_wrap(instance)\n",
    "\n",
    "    def __getattr__(self, name: str) -> Any:\n",
    "        return getattr(self.module_class, name)\n",
    "\n",
    "    def unwrap(self) -> Type[T]:\n",
    "        return self.module_class\n",
    "\n",
    "@overload\n",
    "def auto_wrap(module_or_class: Type[T]) -> WrappedClass[T]: ...\n",
    "\n",
    "@overload\n",
    "def auto_wrap(module_or_class: T) -> WrappedInstance[T]: ...\n",
    "\n",
    "def auto_wrap(module_or_class: Union[T, Type[T]]) -> Union[WrappedInstance[T], WrappedClass[T]]:\n",
    "    '''\n",
    "    This function wraps either a module instance or a module class and returns a type that\n",
    "    preserves the original module's interface plus an additional unwrap method.\n",
    "    '''\n",
    "    if isclass(module_or_class):\n",
    "        return WrappedClass(module_or_class)\n",
    "    else:\n",
    "        wrapped = WrappedInstance(module_or_class)\n",
    "        #NOTE we set the unwrap method to just return module_or_class\n",
    "        wrapped.unwrap = lambda: module_or_class # type: ignore\n",
    "        return cast(WrappedInstance[T], wrapped)\n",
    "\n",
    "class WrappedInstance(HookedRootModule, Generic[T]):\n",
    "    def __init__(self, module: T):\n",
    "        super().__init__()\n",
    "        # NOTE we need to name it in this way to not \n",
    "        # to avoid infinite regress and override \n",
    "        self._module = module\n",
    "        self.hook_point = HookPoint()\n",
    "        self._create_forward()\n",
    "        self._wrap_submodules()\n",
    "        self.setup()\n",
    "\n",
    "    #NOTE we override the nn.Module implementation to use _module only\n",
    "    def named_modules(self, memo: Set[Module] | None = None, prefix: str = '', remove_duplicate: bool = True):\n",
    "        #NOTE BE VERY CAREFUL HERE\n",
    "        \n",
    "        if memo is None:\n",
    "            memo = set()\n",
    "\n",
    "        if self not in memo:\n",
    "            memo.add(self)\n",
    "            yield prefix, self\n",
    "            for name, module in self._module.named_children():\n",
    "                if module not in memo:\n",
    "                    submodule_prefix = prefix + ('.' if prefix else '') + name\n",
    "                    if isinstance(module, WrappedInstance):\n",
    "                        yield from module.named_modules(memo, submodule_prefix)\n",
    "                    else:\n",
    "                        yield submodule_prefix, module\n",
    "                        if hasattr(module, 'named_modules'):\n",
    "                            yield from module.named_modules(memo, submodule_prefix)\n",
    "\n",
    "            if hasattr(self, 'hook_point'):\n",
    "                hook_point_prefix = prefix + ('.' if prefix else '') + 'hook_point'\n",
    "                yield hook_point_prefix, self.hook_point\n",
    "\n",
    "    def unwrap(self) -> T: ...\n",
    "\n",
    "    def _wrap_submodules(self):\n",
    "        for name, submodule in self._module.named_children():\n",
    "            if isinstance(submodule, (nn.ModuleList, nn.ModuleDict, nn.Sequential)):\n",
    "                wrapped_container = type(submodule)() #initialize the container\n",
    "                for i, m in enumerate(submodule):\n",
    "                    wrapped_container.append(auto_wrap(m))\n",
    "                setattr(self._module, name, wrapped_container)\n",
    "            else:\n",
    "                setattr(self._module, name, auto_wrap(submodule))\n",
    "\n",
    "    def _create_forward(self):\n",
    "        original_forward = self._module.forward\n",
    "        original_type_hints = get_type_hints(original_forward)\n",
    "\n",
    "        @functools.wraps(original_forward)\n",
    "        def new_forward(*args: Any, **kwargs: Any) -> Any:\n",
    "            return self.hook_point(original_forward(*args, **kwargs))\n",
    "\n",
    "        new_forward.__annotations__ = original_type_hints\n",
    "        self.forward = new_forward  # Assign to instance, not class\n",
    "\n",
    "    def get_hooks(self):\n",
    "        return [(hook, hook_point) for hook, hook_point in self.hook_dict.items()] \n",
    "\n",
    "args = (1,1)\n",
    "\n",
    "class Linear(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "        self.wtf = nn.Linear(*args)\n",
    "    def forward(self, x):\n",
    "        x = self.wtf(x)\n",
    "        return x\n",
    "\n",
    "class Meta1Linear(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "        self.wtf = Linear(*args)\n",
    "    def forward(self, x):\n",
    "        x = self.wtf(x)\n",
    "        return x\n",
    "\n",
    "class Meta2Linear(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "        self.linear1 = Meta1Linear(*args)\n",
    "        self.linear2 = Meta1Linear(*args)\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "Wrapped = auto_wrap(Meta2Linear)(*args)\n",
    "\n",
    "print(Wrapped.get_hooks())\n",
    "\n",
    "def hook_fn(x, hook=None, hook_name=None):\n",
    "    print('hook_fn CALLED', x, hook_name)\n",
    "\n",
    "Wrapped.run_with_hooks(\n",
    "    torch.rand(1,1),\n",
    "    fwd_hooks=[(hook_name, partial(hook_fn, hook_name=hook_name)) for hook_name, _ in Wrapped.get_hooks()],\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Linear with args (1, 1)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'WrappedLinear2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 56\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m Model, args \u001b[38;5;129;01min\u001b[39;00m TEST_CLASSES:\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTesting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mModel\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with args \u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 56\u001b[0m     \u001b[43mGeneric_test_types\u001b[49m\u001b[43m(\u001b[49m\u001b[43mModel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m     Generic_test_type_hints(Model, args)\n\u001b[1;32m     58\u001b[0m     Generic_test_hook(Model, args)\n",
      "Cell \u001b[0;32mIn[6], line 20\u001b[0m, in \u001b[0;36mGeneric_test_types\u001b[0;34m(Model, args)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(pre_init_Model) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mtype\u001b[39m(post_init_Model), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(pre_init_Model)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m != \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(post_init_Model)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(pre_init_Model\u001b[38;5;241m.\u001b[39munwrap()) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mtype\u001b[39m(post_init_Model\u001b[38;5;241m.\u001b[39munwrap()) , \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(pre_init_Model\u001b[38;5;241m.\u001b[39munwrap())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m != \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(post_init_Model\u001b[38;5;241m.\u001b[39munwrap())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m WrappedLinear1\u001b[38;5;241m.\u001b[39munwrap() \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[43mWrappedLinear2\u001b[49m\u001b[38;5;241m.\u001b[39munwrap()), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mWrappedLinear1\u001b[38;5;241m.\u001b[39munwrap()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m != \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(WrappedLinear2\u001b[38;5;241m.\u001b[39munwrap())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'WrappedLinear2' is not defined"
     ]
    }
   ],
   "source": [
    "#BASIC TESTS\n",
    "\n",
    "class NestedLinear(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(*args)\n",
    "        self.linear2 = nn.Linear(*args)\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def Generic_test_types(Model : Type[T], args):\n",
    "    pre_init_Model = auto_wrap(Model(*args))\n",
    "    post_init_Model = auto_wrap(Model)(*args)\n",
    "\n",
    "    assert type(pre_init_Model) == type(post_init_Model), f\"{type(pre_init_Model)} != {type(post_init_Model)}\"\n",
    "    assert type(pre_init_Model.unwrap()) == type(post_init_Model.unwrap()) , f\"{type(pre_init_Model.unwrap())} != {type(post_init_Model.unwrap())}\"\n",
    "    assert WrappedLinear1.unwrap() == type(WrappedLinear2.unwrap()), f\"{WrappedLinear1.unwrap()} != {type(WrappedLinear2.unwrap())}\"\n",
    "\n",
    "def Generic_test_type_hints(Model : Type[T], args):\n",
    "    pre_init_Model = auto_wrap(Model(*args))\n",
    "    post_init_Model = auto_wrap(Model)(*args)\n",
    "\n",
    "    orig_type_hints = get_type_hints(Model(*args).forward)\n",
    "    wrapped_pre_init_type_hints = get_type_hints(pre_init_Model.forward)\n",
    "    wrapped_post_init_type_hints = get_type_hints(post_init_Model.forward)\n",
    "    assert orig_type_hints == wrapped_pre_init_type_hints, f\"{orig_type_hints} != {wrapped_pre_init_type_hints}\"\n",
    "    assert orig_type_hints == wrapped_post_init_type_hints, f\"{orig_type_hints} != {wrapped_post_init_type_hints}\"\n",
    "\n",
    "def Generic_test_hook(Model : Type[T], args):\n",
    "    WrappedLinear1 = auto_wrap(Model)(*args)\n",
    "    WrappedLinear2 = auto_wrap(Model(*args))\n",
    "\n",
    "    counter = {'data': 0}\n",
    "\n",
    "    def hook_fn(x, hook=None, hook_name=None):\n",
    "        counter['data'] += 1\n",
    "\n",
    "    WrappedLinear1.run_with_hooks(\n",
    "        torch.rand(1,1),\n",
    "        fwd_hooks=[('hook_point', partial(hook_fn, hook_name='hook_point'))],\n",
    "    )\n",
    "\n",
    "    assert counter['data'] == 1\n",
    "    WrappedLinear2.run_with_hooks(\n",
    "        torch.rand(1,1),\n",
    "        fwd_hooks=[('hook_point', partial(hook_fn, hook_name='hook_point'))],\n",
    "    )\n",
    "    assert counter['data'] == 2\n",
    "\n",
    "TEST_CLASSES = [(nn.Linear, (1,1)), (NestedLinear, (1,1))]\n",
    "for Model, args in TEST_CLASSES:\n",
    "    print(f'Testing {Model.__name__} with args {args}')\n",
    "    Generic_test_types(Model, args)\n",
    "    Generic_test_type_hints(Model, args)\n",
    "    Generic_test_hook(Model, args)\n",
    "    print('SUCCESS\\n')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
