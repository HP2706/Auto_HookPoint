{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from inspect import isclass\n",
    "from torch import nn\n",
    "from transformer_lens.hook_points import HookPoint, HookedRootModule\n",
    "from typing import List, Optional, TypeVar, Type, Union, cast, overload\n",
    "from utils import iterate_module\n",
    "from abc import ABC, abstractmethod\n",
    "import torch\n",
    "from functools import partial\n",
    "from fastcore.basics import *\n",
    "from fastcore.foundation import *\n",
    "from torch import nn\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "from typing import TypeVar, Generic, Union, Type, Any, Callable, get_type_hints, ParamSpec, Protocol\n",
    "from inspect import isclass, signature\n",
    "import functools\n",
    "from fastapi import FastAPI\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "class AutoHookedRootModule(HookedRootModule):\n",
    "    '''\n",
    "    This class automatically builds hooks for all modules that are not hooks.\n",
    "    NOTE this does not mean all edges in the graph are hooked only that the outputs of the modules are hooked.\n",
    "    for instance torch.softmax(x) is not hooked but self.softmax(x) would be\n",
    "    ''' \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ModelTest(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bla = nn.ModuleList([nn.Linear(10, 10)])\n",
    "        self.lala = nn.Linear(10, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if isinstance(self, AutoHookedRootModule):\n",
    "            print(f'{self.__class__.__name__}.mod_dict', self.mod_dict)\n",
    "            print(self.bla[0], self.bla[0].hook_dict)\n",
    "        x = self.bla[0].forward(x)\n",
    "        x = self.lala.forward(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "T = TypeVar('T', bound=nn.Module)\n",
    "P = ParamSpec('P')\n",
    "_T = TypeVar(\"_T\", bound=Callable)\n",
    "\n",
    "\n",
    "def same_definition_as_in(t: _T) -> Callable[[Callable], _T]:\n",
    "    def decorator(f: Callable) -> _T:\n",
    "        return f  # type: ignore\n",
    "\n",
    "    return decorator\n",
    "\n",
    "class MyFastAPI(FastAPI):\n",
    "    @same_definition_as_in(FastAPI.__init__)\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs) \n",
    "\n",
    "    @same_definition_as_in(FastAPI.get)\n",
    "    def get(self, *args, **kwargs):\n",
    "        print('get')\n",
    "        return super().get(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type of WrappedLinear <class 'torch.nn.modules.linear.Linear'>\n",
      "get_type_hints(nn.Linear(10, 20).forward) {'input': <class 'torch.Tensor'>, 'return': <class 'torch.Tensor'>}\n",
      "get_type_hints(WrappedLinear.forward) {'input': <class 'torch.Tensor'>, 'return': <class 'torch.Tensor'>}\n",
      "WrappedLinear.forward(torch.randn(10)) tensor([ 0.0644,  0.6165,  0.2268,  0.8688, -0.2364,  0.3074, -0.6121, -0.1512,\n",
      "         0.5288, -1.2037], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from typing import ParamSpec\n",
    "\n",
    "T = TypeVar('T', bound=nn.Module)\n",
    "P = ParamSpec('P')\n",
    "R = TypeVar('R')\n",
    "\n",
    "\n",
    "\n",
    "class WrappedModule(nn.Module, Generic[T]):\n",
    "    def __init__(self, module: T):\n",
    "        super().__init__()\n",
    "        self._module = module\n",
    "        self.hook_point = HookPoint()\n",
    "        self._create_forward()\n",
    "\n",
    "    def _create_forward(self):\n",
    "        original_forward = self._module.forward\n",
    "        original_type_hints = get_type_hints(original_forward)\n",
    "\n",
    "        @functools.wraps(original_forward)\n",
    "        def new_forward(*args: Any, **kwargs: Any) -> Any:\n",
    "            # Remove 'self' from args if it's present\n",
    "            if args and isinstance(args[0], WrappedModule):\n",
    "                args = args[1:]\n",
    "            return self.hook_point(original_forward(*args, **kwargs))\n",
    "\n",
    "        new_forward.__annotations__ = original_type_hints\n",
    "        setattr(self.__class__, 'forward', new_forward)\n",
    "\n",
    "class Wrapper(Generic[T]):\n",
    "    def __init__(self, module: Union[T, Type[T]]):\n",
    "        self.is_class = isclass(module)\n",
    "        self.module = module\n",
    "        if not self.is_class:\n",
    "            self.wrapped = cast(T, WrappedModule(module))\n",
    "\n",
    "    def __call__(self, *args: Any, **kwargs: Any) -> WrappedModule[T]:\n",
    "        if self.is_class:\n",
    "            return WrappedModule(self.module(*args, **kwargs))\n",
    "        else:\n",
    "            return self.wrapped\n",
    "\n",
    "    def unwrap(self) -> Union[T, Type[T]]:\n",
    "        return self.module\n",
    "\n",
    "    def __getattr__(self, name: str) -> Any:\n",
    "        return getattr(self.module, name)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"{self.__class__.__name__}({self.module})\"\n",
    "\n",
    "class Unwrappable(Protocol[T]):\n",
    "    def unwrap(self) -> T: ...\n",
    "\n",
    "class UnwrappableModule(Protocol[T]):\n",
    "    def __getattr__(self, name: str) -> Any: ...\n",
    "    def __call__(self, *args: Any, **kwargs: Any) -> Any: ...\n",
    "    def unwrap(self) -> T: ...\n",
    "\n",
    "\n",
    "class WrappedModuleClass(Generic[T]):\n",
    "    def __init__(self, module_class: Type[T]) -> T: # type: ignore\n",
    "        self.module_class = module_class\n",
    "\n",
    "    def __call__(self, *args: Any, **kwargs: Any) -> UnwrappableModule[T]:\n",
    "        instance = self.module_class(*args, **kwargs)\n",
    "        return auto_wrap(instance)\n",
    "\n",
    "    def __getattr__(self, name: str) -> Any:\n",
    "        return getattr(self.module_class, name)\n",
    "\n",
    "    def unwrap(self) -> Type[T]:\n",
    "        return self.module_class\n",
    "\n",
    "@overload\n",
    "def auto_wrap(module_or_class: Type[T]) -> WrappedModuleClass[T]: ...\n",
    "\n",
    "@overload\n",
    "def auto_wrap(module_or_class: T) -> UnwrappableModule[T]: ...\n",
    "\n",
    "def auto_wrap(module_or_class: Union[T, Type[T]]) -> Union[UnwrappableModule[T], WrappedModuleClass[T]]:\n",
    "    '''\n",
    "    This function wraps either a module instance or a module class and returns a type that\n",
    "    preserves the original module's interface plus an additional unwrap method.\n",
    "    '''\n",
    "    if isclass(module_or_class):\n",
    "        return WrappedModuleClass(module_or_class)\n",
    "    else:\n",
    "        wrapped = WrappedModule(module_or_class)\n",
    "        wrapped.unwrap = lambda: module_or_class  # type: ignore\n",
    "        return cast(UnwrappableModule[T], wrapped)\n",
    "\n",
    "\n",
    "# Usage example:\n",
    "WrappedLinear = auto_wrap(nn.Linear)\n",
    "print(\"type of WrappedLinear\", WrappedLinear.unwrap())\n",
    "print('get_type_hints(nn.Linear(10, 20).forward)', get_type_hints(nn.Linear(10, 20).forward))\n",
    "print('get_type_hints(WrappedLinear.forward)', get_type_hints(WrappedLinear.forward))\n",
    "print('WrappedLinear.forward(torch.randn(10))', WrappedLinear(10, 10).forward(torch.randn(10)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
