{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from inspect import isclass\n",
    "from torch import nn\n",
    "from transformer_lens.hook_points import HookPoint, HookedRootModule\n",
    "from typing import List, Optional, TypeVar, Type, Union, cast, overload\n",
    "from utils import iterate_module\n",
    "from abc import ABC, abstractmethod\n",
    "import torch\n",
    "from functools import partial\n",
    "from fastcore.basics import *\n",
    "from fastcore.foundation import *\n",
    "from torch import nn\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "from typing import TypeVar, Generic, Union, Type, Any, Callable, get_type_hints, ParamSpec, Protocol\n",
    "from inspect import isclass, signature\n",
    "import functools\n",
    "from fastapi import FastAPI\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "class AutoHookedRootModule(HookedRootModule):\n",
    "    '''\n",
    "    This class automatically builds hooks for all modules that are not hooks.\n",
    "    NOTE this does not mean all edges in the graph are hooked only that the outputs of the modules are hooked.\n",
    "    for instance torch.softmax(x) is not hooked but self.softmax(x) would be\n",
    "    ''' \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ModelTest(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bla = nn.ModuleList([nn.Linear(10, 10)])\n",
    "        self.lala = nn.Linear(10, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if isinstance(self, AutoHookedRootModule):\n",
    "            print(f'{self.__class__.__name__}.mod_dict', self.mod_dict)\n",
    "            print(self.bla[0], self.bla[0].hook_dict)\n",
    "        x = self.bla[0].forward(x)\n",
    "        x = self.lala.forward(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "T = TypeVar('T', bound=nn.Module)\n",
    "P = ParamSpec('P')\n",
    "_T = TypeVar(\"_T\", bound=Callable)\n",
    "\n",
    "\n",
    "def same_definition_as_in(t: _T) -> Callable[[Callable], _T]:\n",
    "    def decorator(f: Callable) -> _T:\n",
    "        return f  # type: ignore\n",
    "\n",
    "    return decorator\n",
    "\n",
    "class MyFastAPI(FastAPI):\n",
    "    @same_definition_as_in(FastAPI.__init__)\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs) \n",
    "\n",
    "    @same_definition_as_in(FastAPI.get)\n",
    "    def get(self, *args, **kwargs):\n",
    "        print('get')\n",
    "        return super().get(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import Optional, Set\n",
    "\n",
    "\n",
    "from torch.nn.modules.module import Module\n",
    "\n",
    "\n",
    "T = TypeVar('T', bound=nn.Module)\n",
    "P = ParamSpec('P')\n",
    "R = TypeVar('R')\n",
    "\n",
    "\n",
    "class WrappedClass(Generic[T]):\n",
    "    def __init__(self, module_class: Type[T]) -> T: # type: ignore\n",
    "        self.module_class = module_class\n",
    "\n",
    "    def __call__(self, *args: Any, **kwargs: Any) -> HookedInstance[T]:\n",
    "        instance = self.module_class(*args, **kwargs)\n",
    "        return auto_wrap(instance)\n",
    "\n",
    "    def __getattr__(self, name: str) -> Any:\n",
    "        return getattr(self.module_class, name)\n",
    "\n",
    "    def unwrap(self) -> Type[T]:\n",
    "        '''recursively unwraps the module class'''\n",
    "        for attr in self.module_class.__dict__:\n",
    "            if isinstance(self.module_class.__dict__[attr], WrappedClass):\n",
    "                self.module_class.__dict__[attr] = self.module_class.__dict__[attr].unwrap()\n",
    "        return self.module_class\n",
    "\n",
    "@overload\n",
    "def auto_wrap(module_or_class: Type[T]) -> WrappedClass[T]: ...\n",
    "\n",
    "@overload\n",
    "def auto_wrap(module_or_class: T) -> HookedInstance[T]: ...\n",
    "\n",
    "def auto_wrap(module_or_class: Union[T, Type[T]]) -> Union[HookedInstance[T], WrappedClass[T]]:\n",
    "    '''\n",
    "    This function wraps either a module instance or a module class and returns a type that\n",
    "    preserves the original module's interface plus an additional unwrap method.\n",
    "    '''\n",
    "    if isclass(module_or_class):\n",
    "        return WrappedClass(module_or_class)\n",
    "    else:\n",
    "        wrapped = HookedInstance(module_or_class)\n",
    "        #NOTE we set the unwrap method to just return module_or_class\n",
    "        wrapped.unwrap = lambda: module_or_class # type: ignore\n",
    "        return cast(HookedInstance[T], wrapped)\n",
    "\n",
    "class HookedInstance(HookedRootModule, Generic[T]):\n",
    "    def __init__(self, module: T):\n",
    "        super().__init__()\n",
    "        # NOTE we need to name it in this way to not \n",
    "        # to avoid infinite regress and override \n",
    "        self._module = module\n",
    "        self.hook_point = HookPoint()\n",
    "        self._create_forward()\n",
    "        self._wrap_submodules()\n",
    "        self.setup()\n",
    "\n",
    "    def new_attr_fn(self, name: str) -> Any:\n",
    "        return getattr(self._module, name)\n",
    "\n",
    "    #NOTE we override the nn.Module implementation to use _module only\n",
    "    def named_modules(self, memo: Set[Module] | None = None, prefix: str = '', remove_duplicate: bool = True):\n",
    "        #NOTE BE VERY CAREFUL HERE\n",
    "        \n",
    "        if memo is None:\n",
    "            memo = set()\n",
    "\n",
    "        if self not in memo:\n",
    "            memo.add(self)\n",
    "            yield prefix, self\n",
    "            for name, module in self._module.named_children():\n",
    "                if module not in memo:\n",
    "                    submodule_prefix = prefix + ('.' if prefix else '') + name\n",
    "                    if isinstance(module, HookedInstance):\n",
    "                        yield from module.named_modules(memo, submodule_prefix)\n",
    "                    else:\n",
    "                        yield submodule_prefix, module\n",
    "                        if hasattr(module, 'named_modules'):\n",
    "                            yield from module.named_modules(memo, submodule_prefix)\n",
    "\n",
    "            if hasattr(self, 'hook_point'):\n",
    "                hook_point_prefix = prefix + ('.' if prefix else '') + 'hook_point'\n",
    "                yield hook_point_prefix, self.hook_point\n",
    "\n",
    "    #def unwrap(self) -> T: ...\n",
    "\n",
    "    def _wrap_submodules(self):\n",
    "        for name, submodule in self._module.named_children():\n",
    "            if isinstance(submodule, (nn.ModuleList, nn.Sequential)):\n",
    "                wrapped_container = type(submodule)() #initialize the container\n",
    "                for i, m in enumerate(submodule):\n",
    "                    wrapped_container.append(auto_wrap(m))\n",
    "                setattr(self._module, name, wrapped_container)\n",
    "            elif isinstance(submodule, nn.ModuleDict):\n",
    "                wrapped_container = type(submodule)()\n",
    "                for key, m in submodule.items():\n",
    "                    wrapped_container[key] = auto_wrap(m)\n",
    "                setattr(self._module, name, wrapped_container)\n",
    "            else:\n",
    "                setattr(self._module, name, auto_wrap(submodule))\n",
    "\n",
    "    def _create_forward(self):\n",
    "        original_forward = self._module.forward\n",
    "        original_type_hints = get_type_hints(original_forward)\n",
    "\n",
    "        @functools.wraps(original_forward)\n",
    "        def new_forward(*args: Any, **kwargs: Any) -> Any:\n",
    "            return self.hook_point(original_forward(*args, **kwargs))\n",
    "\n",
    "        new_forward.__annotations__ = original_type_hints\n",
    "        self.forward = new_forward  # Assign to instance, not class\n",
    "\n",
    "    def get_hooks(self):\n",
    "        return [(hook, hook_point) for hook, hook_point in self.hook_dict.items()] \n",
    "\n",
    "args = (1,1)\n",
    "\n",
    "class Linear(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "        self.wtf = nn.Linear(*args)\n",
    "    def forward(self, x):\n",
    "        x = self.wtf(x)\n",
    "        return x\n",
    "\n",
    "class Meta2Linear(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "        self.modules_lst = nn.ModuleDict({'linear1': Linear(*args), 'linear2': Linear(*args)})\n",
    "    def forward(self, x):\n",
    "        x = self.modules_lst['linear1'](x)\n",
    "        x = self.modules_lst['linear2'](x)\n",
    "        return x\n",
    "        #for module in self.modules_lst:\n",
    "        #    x = module(x)\n",
    "        #return x\n",
    "\n",
    "Wrapped = auto_wrap(Meta2Linear(*args))\n",
    "print(Wrapped.unwrap())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BASIC TESTS\n",
    "\n",
    "class NestedLinear(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(*args)\n",
    "        self.linear2 = nn.Linear(*args)\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def Generic_test_types(Model : Type[T], args):\n",
    "    pre_init_Model = auto_wrap(Model(*args))\n",
    "    post_init_Model = auto_wrap(Model)(*args)\n",
    "\n",
    "    assert type(pre_init_Model) == type(post_init_Model), f\"{type(pre_init_Model)} != {type(post_init_Model)}\"\n",
    "    assert type(pre_init_Model.unwrap()) == type(post_init_Model.unwrap()) , f\"{type(pre_init_Model.unwrap())} != {type(post_init_Model.unwrap())}\"\n",
    "    assert WrappedLinear1.unwrap() == type(WrappedLinear2.unwrap()), f\"{WrappedLinear1.unwrap()} != {type(WrappedLinear2.unwrap())}\"\n",
    "\n",
    "def Generic_test_type_hints(Model : Type[T], args):\n",
    "    pre_init_Model = auto_wrap(Model(*args))\n",
    "    post_init_Model = auto_wrap(Model)(*args)\n",
    "\n",
    "    orig_type_hints = get_type_hints(Model(*args).forward)\n",
    "    wrapped_pre_init_type_hints = get_type_hints(pre_init_Model.forward)\n",
    "    wrapped_post_init_type_hints = get_type_hints(post_init_Model.forward)\n",
    "    assert orig_type_hints == wrapped_pre_init_type_hints, f\"{orig_type_hints} != {wrapped_pre_init_type_hints}\"\n",
    "    assert orig_type_hints == wrapped_post_init_type_hints, f\"{orig_type_hints} != {wrapped_post_init_type_hints}\"\n",
    "\n",
    "def Generic_test_hook(Model : Type[T], args):\n",
    "    WrappedLinear1 = auto_wrap(Model)(*args)\n",
    "    WrappedLinear2 = auto_wrap(Model(*args))\n",
    "\n",
    "    counter = {'data': 0}\n",
    "\n",
    "    def hook_fn(x, hook=None, hook_name=None):\n",
    "        counter['data'] += 1\n",
    "\n",
    "    WrappedLinear1.run_with_hooks(\n",
    "        torch.rand(1,1),\n",
    "        fwd_hooks=[('hook_point', partial(hook_fn, hook_name='hook_point'))],\n",
    "    )\n",
    "\n",
    "    assert counter['data'] == 1\n",
    "    WrappedLinear2.run_with_hooks(\n",
    "        torch.rand(1,1),\n",
    "        fwd_hooks=[('hook_point', partial(hook_fn, hook_name='hook_point'))],\n",
    "    )\n",
    "    assert counter['data'] == 2\n",
    "\n",
    "TEST_CLASSES = [(nn.Linear, (1,1)), (NestedLinear, (1,1))]\n",
    "for Model, args in TEST_CLASSES:\n",
    "    print(f'Testing {Model.__name__} with args {args}')\n",
    "    Generic_test_types(Model, args)\n",
    "    Generic_test_type_hints(Model, args)\n",
    "    Generic_test_hook(Model, args)\n",
    "    print('SUCCESS\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "class Bruh(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(1,1)\n",
    "\n",
    "isinstance(Bruh(), nn.Module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'a'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m a \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;241m1\u001b[39m}\n\u001b[0;32m----> 2\u001b[0m \u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43ma\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'a'"
     ]
    }
   ],
   "source": [
    "a = {'a' : 1}\n",
    "a.a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
