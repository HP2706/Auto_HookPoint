{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/ml/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "from inspect import isclass\n",
    "from torch import nn\n",
    "from transformer_lens.hook_points import HookPoint, HookedRootModule\n",
    "from typing import List, Optional, TypeVar, Type, Union, cast, overload\n",
    "from utils import iterate_module\n",
    "from abc import ABC, abstractmethod\n",
    "import torch\n",
    "from functools import partial\n",
    "from fastcore.basics import *\n",
    "from fastcore.foundation import *\n",
    "from torch import nn\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "from typing import TypeVar, Generic, Union, Type, Any, Callable, get_type_hints, ParamSpec, Protocol\n",
    "from inspect import isclass, signature\n",
    "import functools\n",
    "from fastapi import FastAPI\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" \\nclass AutoHookedRootModule(HookedRootModule):\\n    '''\\n    This class automatically builds hooks for all modules that are not hooks.\\n    NOTE this does not mean all edges in the graph are hooked only that the outputs of the modules are hooked.\\n    for instance torch.softmax(x) is not hooked but self.softmax(x) would be\\n    ''' \""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" \n",
    "class AutoHookedRootModule(HookedRootModule):\n",
    "    '''\n",
    "    This class automatically builds hooks for all modules that are not hooks.\n",
    "    NOTE this does not mean all edges in the graph are hooked only that the outputs of the modules are hooked.\n",
    "    for instance torch.softmax(x) is not hooked but self.softmax(x) would be\n",
    "    ''' \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ModelTest(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bla = nn.ModuleList([nn.Linear(10, 10)])\n",
    "        self.lala = nn.Linear(10, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if isinstance(self, AutoHookedRootModule):\n",
    "            print(f'{self.__class__.__name__}.mod_dict', self.mod_dict)\n",
    "            print(self.bla[0], self.bla[0].hook_dict)\n",
    "        x = self.bla[0].forward(x)\n",
    "        x = self.lala.forward(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "T = TypeVar('T', bound=nn.Module)\n",
    "P = ParamSpec('P')\n",
    "_T = TypeVar(\"_T\", bound=Callable)\n",
    "\n",
    "\n",
    "def same_definition_as_in(t: _T) -> Callable[[Callable], _T]:\n",
    "    def decorator(f: Callable) -> _T:\n",
    "        return f  # type: ignore\n",
    "\n",
    "    return decorator\n",
    "\n",
    "class MyFastAPI(FastAPI):\n",
    "    @same_definition_as_in(FastAPI.__init__)\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs) \n",
    "\n",
    "    @same_definition_as_in(FastAPI.get)\n",
    "    def get(self, *args, **kwargs):\n",
    "        print('get')\n",
    "        return super().get(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('linear1.hook_point', HookPoint()), ('linear2.hook_point', HookPoint()), ('hook_point', HookPoint())]\n",
      "hook_fn CALLED tensor([[0.1884]], grad_fn=<AddmmBackward0>) linear1.hook_point\n",
      "hook_fn CALLED tensor([[0.2085]], grad_fn=<AddmmBackward0>) linear2.hook_point\n",
      "hook_fn CALLED tensor([[0.2085]], grad_fn=<AddmmBackward0>) hook_point\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.2085]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from typing import Optional, Set\n",
    "\n",
    "\n",
    "from torch.nn.modules.module import Module\n",
    "\n",
    "\n",
    "T = TypeVar('T', bound=nn.Module)\n",
    "P = ParamSpec('P')\n",
    "R = TypeVar('R')\n",
    "\n",
    "\n",
    "class WrappedClass(Generic[T]):\n",
    "    def __init__(self, module_class: Type[T]) -> T: # type: ignore\n",
    "        self.module_class = module_class\n",
    "\n",
    "    def __call__(self, *args: Any, **kwargs: Any) -> WrappedInstance[T]:\n",
    "        instance = self.module_class(*args, **kwargs)\n",
    "        return auto_wrap(instance)\n",
    "\n",
    "    def __getattr__(self, name: str) -> Any:\n",
    "        return getattr(self.module_class, name)\n",
    "\n",
    "    def unwrap(self) -> Type[T]:\n",
    "        return self.module_class\n",
    "\n",
    "\n",
    "@overload\n",
    "def auto_wrap(module_or_class: Type[T]) -> WrappedClass[T]: ...\n",
    "\n",
    "@overload\n",
    "def auto_wrap(module_or_class: T) -> WrappedInstance[T]: ...\n",
    "\n",
    "def auto_wrap(module_or_class: Union[T, Type[T]]) -> Union[WrappedInstance[T], WrappedClass[T]]:\n",
    "    '''\n",
    "    This function wraps either a module instance or a module class and returns a type that\n",
    "    preserves the original module's interface plus an additional unwrap method.\n",
    "    '''\n",
    "    if isclass(module_or_class):\n",
    "        return WrappedClass(module_or_class)\n",
    "    else:\n",
    "        wrapped = WrappedInstance(module_or_class)\n",
    "        #NOTE we set the unwrap method to just return module_or_class\n",
    "        wrapped.unwrap = lambda: module_or_class # type: ignore\n",
    "        return cast(WrappedInstance[T], wrapped)\n",
    "\n",
    "\n",
    "class WrappedInstance(HookedRootModule, Generic[T]):\n",
    "    def __init__(self, module: T):\n",
    "        super().__init__()\n",
    "        # NOTE we need to name it in this way to not \n",
    "        # to avoid infinite regress and override \n",
    "        self._module = module\n",
    "        self.hook_point = HookPoint()\n",
    "        self._create_forward()\n",
    "        self._wrap_submodules()\n",
    "        self.setup()\n",
    "\n",
    "    #NOTE we override the nn.Module implementation to use _module only\n",
    "    def named_modules(self, memo: Set[Module] | None = None, prefix: str = '', remove_duplicate: bool = True):\n",
    "        #NOTE BE VERY CAREFUL HERE\n",
    "        for i, (name, module) in enumerate(self._module.named_children()):\n",
    "            if isinstance(module, WrappedInstance):\n",
    "                #NOTE named_modules() returns empty list for WrappedInstance\n",
    "                for sub_name, sub_module in module.named_children(): \n",
    "                    if sub_name.startswith('_'):\n",
    "                        sub_name = sub_name[1:]\n",
    "                    yield f'{name}.{sub_name}', sub_module\n",
    "            elif isinstance(module, HookPoint):\n",
    "                print('is hook point', name, module)\n",
    "                yield name, module\n",
    "            else:\n",
    "                # Otherwise, yield the module as is\n",
    "                yield name, module\n",
    "        \n",
    "        if hasattr(self, 'hook_point'):\n",
    "            yield 'hook_point', self.hook_point\n",
    "        else:\n",
    "            print('no hook point')\n",
    "\n",
    "    def unwrap(self) -> T: ...\n",
    "\n",
    "    def _wrap_submodules(self):\n",
    "        for name, submodule in self._module.named_children():\n",
    "            if isinstance(submodule, (nn.ModuleList, nn.ModuleDict, nn.Sequential)):\n",
    "                wrapped_container = type(submodule)() #initialize the container\n",
    "                for i, m in enumerate(submodule):\n",
    "                    wrapped_container.append(auto_wrap(m))\n",
    "                setattr(self._module, name, wrapped_container)\n",
    "            else:\n",
    "                setattr(self._module, name, auto_wrap(submodule))\n",
    "\n",
    "    def _create_forward(self):\n",
    "        original_forward = self._module.forward\n",
    "        original_type_hints = get_type_hints(original_forward)\n",
    "\n",
    "        @functools.wraps(original_forward)\n",
    "        def new_forward(*args: Any, **kwargs: Any) -> Any:\n",
    "            return self.hook_point(original_forward(*args, **kwargs))\n",
    "\n",
    "        new_forward.__annotations__ = original_type_hints\n",
    "        self.forward = new_forward  # Assign to instance, not class\n",
    "\n",
    "    def get_hooks(self):\n",
    "        return [(hook, hook_point) for hook, hook_point in self.hook_dict.items()] \n",
    "\n",
    "args = (1,1)\n",
    "\n",
    "class NestedLinear(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(*args)\n",
    "        self.linear2 = nn.Linear(*args)\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "WrappedLinear1 = auto_wrap(NestedLinear)(*args)\n",
    "\n",
    "\n",
    "def hook_fn(x, hook=None, hook_name=None):\n",
    "    print('hook_fn CALLED', x, hook_name)\n",
    "\n",
    "print(WrappedLinear1.get_hooks())\n",
    "\n",
    "WrappedLinear1.run_with_hooks(\n",
    "    torch.rand(1,1),\n",
    "    fwd_hooks=[(hook_name, partial(hook_fn, hook_name=hook_name)) for hook_name, _ in WrappedLinear1.get_hooks()],\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Linear with args (1, 1)\n",
      "SUCCESS\n",
      "\n",
      "Testing NestedLinear with args (1, 1)\n",
      "SUCCESS\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#BASIC TESTS\n",
    "\n",
    "class NestedLinear(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(*args)\n",
    "        self.linear2 = nn.Linear(*args)\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def Generic_test_types(Model : Type[T], args):\n",
    "    pre_init_Model = auto_wrap(Model(*args))\n",
    "    post_init_Model = auto_wrap(Model)(*args)\n",
    "\n",
    "    assert type(pre_init_Model) == type(post_init_Model), f\"{type(pre_init_Model)} != {type(post_init_Model)}\"\n",
    "    assert type(pre_init_Model.unwrap()) == type(post_init_Model.unwrap()) , f\"{type(pre_init_Model.unwrap())} != {type(post_init_Model.unwrap())}\"\n",
    "    assert WrappedLinear1.unwrap() == type(WrappedLinear2.unwrap()), f\"{WrappedLinear1.unwrap()} != {type(WrappedLinear2.unwrap())}\"\n",
    "\n",
    "def Generic_test_type_hints(Model : Type[T], args):\n",
    "    pre_init_Model = auto_wrap(Model(*args))\n",
    "    post_init_Model = auto_wrap(Model)(*args)\n",
    "\n",
    "    orig_type_hints = get_type_hints(Model(*args).forward)\n",
    "    wrapped_pre_init_type_hints = get_type_hints(pre_init_Model.forward)\n",
    "    wrapped_post_init_type_hints = get_type_hints(post_init_Model.forward)\n",
    "    assert orig_type_hints == wrapped_pre_init_type_hints, f\"{orig_type_hints} != {wrapped_pre_init_type_hints}\"\n",
    "    assert orig_type_hints == wrapped_post_init_type_hints, f\"{orig_type_hints} != {wrapped_post_init_type_hints}\"\n",
    "\n",
    "def Generic_test_hook(Model : Type[T], args):\n",
    "    WrappedLinear1 = auto_wrap(Model)(*args)\n",
    "    WrappedLinear2 = auto_wrap(Model(*args))\n",
    "\n",
    "    counter = {'data': 0}\n",
    "\n",
    "    def hook_fn(x, hook=None, hook_name=None):\n",
    "        counter['data'] += 1\n",
    "\n",
    "    WrappedLinear1.run_with_hooks(\n",
    "        torch.rand(1,1),\n",
    "        fwd_hooks=[('hook_point', partial(hook_fn, hook_name='hook_point'))],\n",
    "    )\n",
    "\n",
    "    assert counter['data'] == 1\n",
    "    WrappedLinear2.run_with_hooks(\n",
    "        torch.rand(1,1),\n",
    "        fwd_hooks=[('hook_point', partial(hook_fn, hook_name='hook_point'))],\n",
    "    )\n",
    "    assert counter['data'] == 2\n",
    "\n",
    "TEST_CLASSES = [(nn.Linear, (1,1)), (NestedLinear, (1,1))]\n",
    "for Model, args in TEST_CLASSES:\n",
    "    print(f'Testing {Model.__name__} with args {args}')\n",
    "    Generic_test_types(Model, args)\n",
    "    Generic_test_type_hints(Model, args)\n",
    "    Generic_test_hook(Model, args)\n",
    "    print('SUCCESS\\n')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
